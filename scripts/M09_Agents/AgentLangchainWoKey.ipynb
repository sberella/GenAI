{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8214420c-1936-472c-930e-1f4c9efb3809",
   "metadata": {},
   "source": [
    "# GenAI with Python: Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3331a72",
   "metadata": {},
   "source": [
    "- Large Language Model\n",
    "- Prompt Engineering\n",
    "- Tools\n",
    "- Routing\n",
    "- Typensicherheit\n",
    "- LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d45e3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install groq\n",
    "#pip install langchain --> 0.2.14\n",
    "#pip install duckduckgo-search --> 6.2.12\n",
    "#pip install instructor pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c834c3",
   "metadata": {},
   "source": [
    "# Large Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebd7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "# Groq API Key\n",
    "groq_api_key = \"your_groq_key\"\n",
    "\n",
    "# Initialisiere Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Modelname\n",
    "model_name = \"llama3-8b-8192\"\n",
    "\n",
    "# Rollenvergabe für Konversation\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a knowledgeable assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where did Bashar flee from Syria in December 2024?\"}\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ad05cb",
   "metadata": {},
   "source": [
    "Bei der Interaktion mit einem LLM-Chatbot gibt es drei Rollen:\n",
    "\n",
    "„role\": ‚system‘ - wird verwendet, um dem Modell Kernanweisungen zu übermitteln, wie die Konversation verlaufen soll  \n",
    "„Rolle\": ‚user‘ - wird für die Fragen des Benutzers verwendet  \n",
    "„Rolle\": ‚assistant‘ - ist die Antwort des Modells  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825617e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant Response: I think there may be some confusion. As of December 2024, Bashar al-Assad is still the President of Syria, and there has been no credible information or reports of him fleeing the country. It's possible that you may be referring to a different event or scenario. If you have any more context or clarification, I'd be happy to help.\n"
     ]
    }
   ],
   "source": [
    "# Groq chat completion API\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=200,  # Token limit je nach Use Case anpassen\n",
    "    temperature=0.7,  # von 0 - 1, je höher desto kreativer\n",
    "    top_p=1.0  # Nucleus sampling steuert zusätzlich die Variabilität der Ausgabe, ähnlich zu Temperature. Es bestimmt, wie viele der wahrscheinlichsten nächsten Token für die Generierung \n",
    "    #berücksichtigt werden. Bei 1 werden alle möglichen Token berücksichtigt. Je kleiner top_p, desto deterministischer und weniger kreativ wird die Antwort\n",
    ")\n",
    "\n",
    "# Output the response\n",
    "print(\"Assistant Response:\", response.choices[0].message.content) # Groq API liefert Antworten in einer Liste von möglichen Optionen (Choices) zurück. Das erlaubt es, mehrere Antwortvarianten zu generieren,\n",
    "#falls gewünscht.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e4e75",
   "metadata": {},
   "source": [
    "# Prompt Engineering\n",
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd1e99",
   "metadata": {},
   "source": [
    "Beinhaltet die Erstellung eines schrittweisen Argumentationsprozesses, um zu einer Schlussfolgerung zu gelangen. Das Modell wird zum „lauten Denken“ angeregt, indem die logischen Schritte, die zur endgültigen Antwort führen, explizit dargelegt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0085643c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response with Chain of Thought reasoning:\n",
      "I'd like to clarify that Bashar al-Assad is the current President of Syria, and I'll provide a step-by-step reason before answering your question.\n",
      "\n",
      "Step 1: Check the current date\n",
      "Since you mentioned December 2024, I'll assume we're talking about a hypothetical scenario in the future. However, I'll proceed with the question as if the information is accurate.\n",
      "\n",
      "Step 2: Verify Bashar al-Assad's whereabouts in December 2024\n",
      "Bashar al-Assad has been the President of Syria since 2000, and as far as my knowledge cutoff is concerned, there has been no credible information suggesting he has fled the country.\n",
      "\n",
      "Step 3: Consider the possibility of a hypothetical scenario\n",
      "If we assume that Bashar al-Assad did flee Syria in December 2024, I would need more context or credible sources to determine his exact location.\n",
      "\n",
      "Step 4: Provide an answer based on the available information\n",
      "Since there is no credible information or evidence suggesting Bashar al-Assad fled Syria in December 2024, I cannot provide a specific location where he fled to. If new information becomes available, I'll be happy to reassess and provide an answer.\n",
      "\n",
      "In conclusion, based on my knowledge and the available information, I cannot provide a specific location where Bashar al-Assad fled from Syria in December 2024. This is because there is no credible evidence or information suggesting he left the country. If you have any further questions or clarification, I'll be happy to help.\n"
     ]
    }
   ],
   "source": [
    "# Prompt Engineering\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant who reasons step by step before answering any question. Always explain your reasoning clearly before providing a final answer.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Where did Bashar flee from Syria in December 2024? Please reason step by step before providing the answer.\"}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=model_name,\n",
    "    messages=messages,\n",
    "    max_tokens=400,  \n",
    "    temperature=0.7,  \n",
    "    top_p=1.0\n",
    ")\n",
    "\n",
    "# Output the response\n",
    "print(\"Response with Chain of Thought reasoning:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a974c",
   "metadata": {},
   "source": [
    "### Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defd4bf4",
   "metadata": {},
   "source": [
    "Langchain ist ideal für die Erstellung umfangreicher LLM-gestützter Anwendungen, die komplexe Interaktionen und Arbeitsabläufe erfordern. Es bietet eine umfassende Reihe von Tools, die sich auf die Erstellung komplexer Arbeitsabläufe konzentrieren. Es basiert auf einer „Kette von Komponenten“, die verschiedene Verarbeitungsschritte ermöglicht, wobei jeder Schritt verschiedene LLMs beinhalten kann.  \n",
    "Wir nutzen die von der Community zur Verfügung gestellte Bibliothek für die Internetsuche mittels DuckDuckGo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bb32963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing query: Where did Bashar flee from Syria in December 2024?\n",
      "Result: {'result': \"Ousted Syrian President Bashar Assad flees to parts unknown as rebels declare country is 'free of the tyrant' ... By Bill Hutchinson. December 8, 2024, 2:40 PM. 3:30. ... Syria, Dec. 8, 2024. Published On 8 Dec 2024 8 Dec 2024 In the early hours of Sunday morning, opposition forces declared Syria liberated from the rule of President Bashar al-Assad as opposition forces surged into the ... — Thomas van Linge (@ThomasVLinge) December 8, 2024 'Syria is free': By 6am on December 8, the fighters declared Damascus liberated, confirming that Bashar al-Assad had fled the country. Syrian President Bashar al-Assad arrived in Russia on Sunday after rebels seized control of Damascus, Russian state media said—a pivotal moment in the years-long civil war against the regime ... A giant banner of Syrian President Bashar Assad hangs on the facade of a building, as pedestrian walk through an the empty streets of Damascus, Syria, on Dec. 7, 2024. Omar Sanadiki / AP\"}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun  # Importiert das DuckDuckGo-Suchwerkzeug aus LangChain\n",
    "import json  # Importiert das JSON-Modul zur Formatierung und Verarbeitung von Daten\n",
    "\n",
    "def search_duckduckgo(query: str):\n",
    "    \"\"\"\n",
    "    Durchführen einer Suche bei DuckDuckGo mithilfe des LangChain DuckDuckGoSearchRun-Tools.\n",
    "    \n",
    "    Parameter:\n",
    "        query (str): Die Suchanfrage, die an DuckDuckGo gesendet wird.\n",
    "        \n",
    "    Rückgabewert:\n",
    "        str: Ein JSON-String, der entweder das Ergebnis oder eine Fehlermeldung enthält.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Erstelle eine Instanz des DuckDuckGo-Suchtools\n",
    "        search_tool = DuckDuckGoSearchRun()\n",
    "        \n",
    "        # Führe die Suche mit der angegebenen Anfrage aus\n",
    "        result = search_tool.run(query)\n",
    "        \n",
    "        # Gib das Ergebnis als JSON-String zurück\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        # Bei einem Fehler wird die Fehlermeldung als JSON zurückgegeben\n",
    "        return json.dumps({\"error\": f\"An error occurred: {str(e)}\"})\n",
    "\n",
    "# Testfall-Funktion zum Testen der DuckDuckGo-Suche\n",
    "def test_search_duckduckgo():\n",
    "    # Eine Liste von Test-Suchanfragen\n",
    "    test_queries = [\n",
    "        \"Where did Bashar flee from Syria in December 2024?\"  # Beispielhafte Suchanfrage\n",
    "    ]\n",
    "\n",
    "    # Iteriere durch alle Test-Suchanfragen und teste die Funktion\n",
    "    for query in test_queries:\n",
    "        print(f\"Testing query: {query}\")  # Zeige die aktuelle Testanfrage an\n",
    "        \n",
    "        # Führe die Suche aus und erhalte das Ergebnis\n",
    "        result = search_duckduckgo(query)\n",
    "        \n",
    "        # Parsen des Ergebnisses aus dem JSON-Format\n",
    "        parsed_result = json.loads(result)\n",
    "        \n",
    "        # Zeige das Ergebnis der Suche an\n",
    "        print(\"Result:\", parsed_result)\n",
    "        print(\"-\" * 50)  # Trennlinie zur Übersichtlichkeit\n",
    "\n",
    "# Aufruf der Testfunktion\n",
    "test_search_duckduckgo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5044e318",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b62d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"\n",
    "    Verarbeite einen mathematischen Ausdruck und gebe das Ergebnis im JSON-Format zurück.\n",
    "    :param expression: Ein mathematischer Ausdruck als Zeichenkette (String)\n",
    "    :return: JSON-Darstellung des Ergebnisses oder einer Fehlermeldung\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = eval(expression)  # Berechne den mathematischen Ausdruck mit eval()\n",
    "        return json.dumps({\"result\": result})  # Gib das Ergebnis als JSON aus\n",
    "    except:\n",
    "        # Bei einem Fehler (z.B. ungültiger Ausdruck) gib eine Fehlermeldung zurück\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cc0fdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_conversation(user_prompt):\n",
    "    # Initialisiere die Konversation mit system- und user-Nachrichten\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",  # Rolle des Systems, das als Rechner-Assistent fungiert\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",  # Rolle des Benutzers, der eine Eingabe macht\n",
    "            \"content\": user_prompt,  # Benutzer-Prompt als Inhalt\n",
    "        }\n",
    "    ]\n",
    "    # Definiere das vorhandene Tool, das für die Berechnung verwendet wird\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",  # Typ des Tools: eine Funktion\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",  # Name der zu verwendenden Funktion\n",
    "                \"description\": \"Evaluate a mathematical expression\",  # Beschreibung des Tools\n",
    "                \"parameters\": {  # Parameter, die an die Funktion übergeben werden\n",
    "                    \"type\": \"object\",  # Typ des Parameters: ein Objekt\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {\n",
    "                            \"type\": \"string\",  # Der Ausdruck muss als Zeichenkette vorliegen\n",
    "                            \"description\": \"The mathematical expression to evaluate\",  # Beschreibung des Parameters\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],  # Der Parameter \"expression\" ist erforderlich\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Initial Groq API Aufruf\n",
    "    response = client.chat.completions.create(\n",
    "        model=model, \n",
    "        messages=messages, # Konversationshistorie\n",
    "        stream=False,\n",
    "        tools=tools, \n",
    "        tool_choice=\"auto\", # Das LLM entscheidet welche Tools verwendet werden\n",
    "        max_tokens=4096 # Maximum Tokens für die Antwort\n",
    "    )\n",
    "    # Extrahiert die Antwort und alle Toolaufrufe\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        # Definiere die verfügbaren Tools, die vom LLM aufgerfufen werden können\n",
    "        available_functions = {\n",
    "            \"calculate\": calculate,\n",
    "        }\n",
    "        # Füge die LLM Antwort der Konversation hinzu\n",
    "        messages.append(response_message)\n",
    "\n",
    "        # Verarbeite jeden Toolaufruf\n",
    "        for tool_call in tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            function_to_call = available_functions[function_name]\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Toolaufruf und Antwort erhalten\n",
    "            function_response = function_to_call(\n",
    "                expression=function_args.get(\"expression\")\n",
    "            )\n",
    "            # Toolantwort der Konversation hinzufügen\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id, \n",
    "                    \"role\": \"tool\", # Indiziert die Toolverwendung\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        # Zweiter API Aufruf inkl. aktualisierter Konversation\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages\n",
    "        )\n",
    "        # Liefern der finalen Antwort\n",
    "        return second_response.choices[0].message.content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3a495a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It looks like I've done the calculation!\n",
      "\n",
      "According to the calculation, 25 multiplied by 4 is 100. Then, adding 10 to that result gives us... 110!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3-8b-8192\"\n",
    "user_prompt = \"What is 25 * 4 + 10?\"\n",
    "print(run_conversation(user_prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c49784f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I apologize for the mistake! It seems like I need to redefine my purpose.\n",
      "\n",
      "Hello! I'm a calculator assistant. I can help you perform mathematical calculations. You can ask me a math problem, and I'll do my best to solve it.\n",
      "\n",
      "What kind of calculation would you like to do? Addition, subtraction, multiplication, division, or something more complex like exponentiation, roots, or trigonometry?\n"
     ]
    }
   ],
   "source": [
    "user_prompt = \"Hello\"\n",
    "print(run_conversation(user_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c07edf",
   "metadata": {},
   "source": [
    "# Routing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f296bda1",
   "metadata": {},
   "source": [
    "Im Folgenden erweitern wir das Berechnungsprogramm, so dass ein Routing-System unsere Anfrage an Llama 3 70B weiterleitet, wenn die Benutzeranfrage das Tool nicht benötigt  \n",
    "Dieses Routing basiert auf LLM basiertem \"Reasoning und Action (ReAct)\" und entspricht der Funktionsweise eines Agenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cc35c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Definiere Modelle\n",
    "ROUTING_MODEL = \"llama3-70b-8192\"\n",
    "TOOL_USE_MODEL = \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "GENERAL_MODEL = \"llama3-70b-8192\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b36cd390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(expression):\n",
    "    \"\"\"Verarbeite einen mathematischen Ausdruck\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except:\n",
    "        return json.dumps({\"error\": \"Invalid expression\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7125482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_query(query):\n",
    "    \"\"\"Routing-Logik, um LLM entscheiden zu lassen, ob Tools benötigt werden\"\"\"\n",
    "    routing_prompt = f\"\"\"\n",
    "    Given the following user query, determine if any tools are needed to answer it.\n",
    "    If a calculation tool is needed, respond with 'TOOL: CALCULATE'.\n",
    "    If no tools are needed, respond with 'NO TOOL'.\n",
    "\n",
    "    User query: {query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=ROUTING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a routing assistant. Determine if tools are needed based on the user query.\"},\n",
    "            {\"role\": \"user\", \"content\": routing_prompt}\n",
    "        ],\n",
    "        max_tokens=20  # Kurze Antwort reicht aus\n",
    "    )\n",
    "    \n",
    "    routing_decision = response.choices[0].message.content.strip()\n",
    "    \n",
    "    if \"TOOL: CALCULATE\" in routing_decision:\n",
    "        return \"calculate tool needed\"\n",
    "    else:\n",
    "        return \"no tool needed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "87c2eb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_tool(query):\n",
    "    \"\"\"Tool, damit das Modell eine Berechnung ausführen kann.\"\"\"\n",
    "    \n",
    "    # Definiere die Nachrichten für das Modell. Das Modell wird als Taschenrechner-Assistent eingesetzt.\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a calculator assistant. Use the calculate function to perform mathematical operations and provide the results.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,  # Die Benutzeranfrage, z.B. eine mathematische Formel.\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Definiere die Tools, die das Modell verwenden kann.\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",  # Gibt an, dass es sich um eine Funktion handelt.\n",
    "            \"function\": {\n",
    "                \"name\": \"calculate\",  # Name der zu verwendenden Funktion.\n",
    "                \"description\": \"Evaluate a mathematical expression\",  # Beschreibung der Funktion.\n",
    "                \"parameters\": {  # Parameter, die für die Funktion benötigt werden.\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"expression\": {  # Der Parameter \"expression\" wird als mathematische Formel übergeben.\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The mathematical expression to evaluate\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"expression\"],  # Der Parameter \"expression\" ist zwingend erforderlich.\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Anfrage an das Modell senden: LLM bestimmt, ob ein Tool (calculate) verwendet werden soll.\n",
    "    response = client.chat.completions.create(\n",
    "        model=TOOL_USE_MODEL,  # Modell, das Tools nutzen kann.\n",
    "        messages=messages,  # Die Konversation, inklusive System- und Benutzernachrichten.\n",
    "        tools=tools,  # Die definierten Tools (hier: \"calculate\").\n",
    "        tool_choice=\"auto\",  # Das Modell entscheidet automatisch, ob das Tool verwendet wird.\n",
    "        max_tokens=4096  # Maximale Anzahl der Tokens für die Antwort.\n",
    "    )\n",
    "    \n",
    "    # Extrahiere die Antwort des Modells aus der API-Antwort.\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls  # Tool-Aufrufe, die vom Modell bestimmt wurden.\n",
    "\n",
    "    # Prüfen, ob ein Tool-Aufruf erforderlich ist.\n",
    "    if tool_calls:\n",
    "        messages.append(response_message)  # Füge die Antwort des Modells zu den Nachrichten hinzu.\n",
    "        \n",
    "        # Verarbeite jeden Tool-Aufruf.\n",
    "        for tool_call in tool_calls:\n",
    "            # Lade die Argumente, die das Modell an die Funktion übergeben möchte.\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            # Führe die berechnete Funktion aus und hole das Ergebnis.\n",
    "            function_response = calculate(function_args.get(\"expression\"))\n",
    "            \n",
    "            # Füge die Antwort des Tools zur Nachrichtenhistorie hinzu.\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,  # ID des Tool-Aufrufs\n",
    "                    \"role\": \"tool\",  # Rolle des Tools\n",
    "                    \"name\": \"calculate\",  # Name des aufgerufenen Tools\n",
    "                    \"content\": function_response,  # Ergebnis der Berechnung\n",
    "                }\n",
    "            )\n",
    "        \n",
    "        # Starte eine zweite Anfrage mit aktualisierter Nachrichtenhistorie,\n",
    "        # damit das Modell die endgültige Antwort generieren kann.\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=TOOL_USE_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        return second_response.choices[0].message.content  # Rückgabe der endgültigen Antwort.\n",
    "    \n",
    "    # Falls keine Tool-Aufrufe erforderlich waren, Rückgabe der ursprünglichen Antwort.\n",
    "    return response_message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68bfdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_general(query):\n",
    "    \"\"\"Verwende das allgemeine Modell, um die Anfrage zu beantworten, wenn kein Tool benötigt wird.\"\"\"\n",
    "    \n",
    "    # Anfrage an das Modell senden. Es werden allgemeine Informationen ohne Tool-Aufrufe generiert.\n",
    "    response = client.chat.completions.create(\n",
    "        model=GENERAL_MODEL,  # Das allgemeine Modell, das keine Tools verwendet.\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},  # Systemrolle: Modellverhalten definieren\n",
    "            {\"role\": \"user\", \"content\": query}  # Benutzeranfrage an das Modell\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extrahiere die Antwort des Modells aus der API-Antwort.\n",
    "    return response.choices[0].message.content  # Rückgabe der Antwort des Modells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d31236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(query):\n",
    "    \"\"\"Verarbeite die Anfrage und leite sie an das entsprechende Modell weiter\"\"\"\n",
    "    \n",
    "    # Routenentscheidung: Bestimme, welches Modell oder Tool verwendet werden soll.\n",
    "    route = route_query(query)  # Die Funktion 'route_query' analysiert die Anfrage und gibt die Route zurück.\n",
    "    \n",
    "    # Überprüfe die Routing-Entscheidung:\n",
    "    if route == \"calculate\":  # Falls die Entscheidung lautet, ein Berechnungstool zu verwenden\n",
    "        response = run_with_tool(query)  # Führe das Berechnungstool aus, um die Anfrage zu beantworten.\n",
    "    else:\n",
    "        response = run_general(query)  # Andernfalls verwende das allgemeine Modell zur Beantwortung der Anfrage.\n",
    "    \n",
    "    # Rückgabe eines Dictionaries mit:\n",
    "    # 1. Der ursprünglichen Anfrage (query)\n",
    "    # 2. Der gewählten Route (route: \"calculate\" oder \"no tool needed\")\n",
    "    # 3. Der generierten Antwort (response)\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"route\": route,\n",
    "        \"response\": response\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b97c1268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of the Netherlands?\n",
      "Route: no tool needed\n",
      "Response: The capital of the Netherlands is Amsterdam!\n",
      "\n",
      "Query: Calculate 25 * 4 + 10\n",
      "Route: calculate tool needed\n",
      "Response: To calculate this, I'll follow the order of operations (PEMDAS):\n",
      "\n",
      "1. Multiply 25 and 4: 25 * 4 = 100\n",
      "2. Add 10 to the result: 100 + 10 = 110\n",
      "\n",
      "So the final answer is 110!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"What is the capital of the Netherlands?\",\n",
    "    \"Calculate 25 * 4 + 10\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    result = process_query(query)\n",
    "    print(f\"Query: {result['query']}\")\n",
    "    print(f\"Route: {result['route']}\")\n",
    "    print(f\"Response: {result['response']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175e9a3",
   "metadata": {},
   "source": [
    "### Zusammenführung des Programms in einer Zelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d90f0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of the Netherlands?\n",
      "Route: no tool needed\n",
      "Response: The capital of the Netherlands is Amsterdam!\n",
      "\n",
      "Query: Where did Bashar flee from Syria in December 2024\n",
      "Route: search tool needed\n",
      "Response: It seems there was an error in fetching the information. Could you please provide more details or clarify the question?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "import requests\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "\n",
    "# Define models\n",
    "ROUTING_MODEL = \"llama3-70b-8192\"\n",
    "TOOL_USE_MODEL = \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "GENERAL_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "\n",
    "# Use LangChain's DuckDuckGoSearchRun tool\n",
    "def search_duckduckgo(query: str):\n",
    "    \"\"\"Search DuckDuckGo using LangChain's DuckDuckGoSearchRun tool.\"\"\"\n",
    "    try:\n",
    "        search_tool = DuckDuckGoSearchRun()\n",
    "        result = search_tool.run(query)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"An error occurred: {str(e)}\"})\n",
    "\n",
    "def route_query(query):\n",
    "    \"\"\"Routing logic to let LLM decide if a search tool is needed.\"\"\"\n",
    "    routing_prompt = f\"\"\"\n",
    "    Given the following user query, determine if any tools are needed to answer it.\n",
    "    If a search tool is needed, respond with 'TOOL: SEARCH'.\n",
    "    If no tools are needed, respond with 'NO TOOL'.\n",
    "\n",
    "    User query: {query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=ROUTING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a routing assistant. Determine if tools are needed based on the user query.\"},\n",
    "            {\"role\": \"user\", \"content\": routing_prompt}\n",
    "        ],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    \n",
    "    routing_decision = response.choices[0].message.content.strip()\n",
    "    \n",
    "    if \"TOOL: SEARCH\" in routing_decision:\n",
    "        return \"search tool needed\"\n",
    "    else:\n",
    "        return \"no tool needed\"\n",
    "\n",
    "def run_with_tool(query):\n",
    "    \"\"\"Use the DuckDuckGo tool to perform the search.\"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a search assistant. Use the search function to fetch results and provide answers.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query,\n",
    "        }\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_duckduckgo\",\n",
    "                \"description\": \"Search DuckDuckGo for answers to a query.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The search query to look up on DuckDuckGo.\",\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=TOOL_USE_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "    if tool_calls:\n",
    "        messages.append(response_message)\n",
    "        for tool_call in tool_calls:\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = search_duckduckgo(function_args.get(\"query\"))\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": \"search_duckduckgo\",\n",
    "                    \"content\": function_response,\n",
    "                }\n",
    "            )\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=TOOL_USE_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        return second_response.choices[0].message.content\n",
    "    return response_message.content\n",
    "\n",
    "def run_general(query):\n",
    "    \"\"\"Use the general model to answer the query since no tool is needed.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=GENERAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_query(query):\n",
    "    \"\"\"Process the query and route it to the appropriate model.\"\"\"\n",
    "    route = route_query(query)\n",
    "    if route == \"search tool needed\":\n",
    "        response = run_with_tool(query)\n",
    "    else:\n",
    "        response = run_general(query)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"route\": route,\n",
    "        \"response\": response\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    queries = [\n",
    "        \"What is the capital of the Netherlands?\",\n",
    "        \"Where did Bashar flee from Syria in December 2024\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        result = process_query(query)\n",
    "        print(f\"Query: {result['query']}\")\n",
    "        print(f\"Route: {result['route']}\")\n",
    "        print(f\"Response: {result['response']}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc06fdb",
   "metadata": {},
   "source": [
    "### Eingabemaske"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eec4f2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Willkommen beim intelligenten Agenten! ###\n",
      "Geben Sie 'exit' ein, um das Programm zu beenden.\n",
      "\n",
      "\n",
      "Ihre Anfrage: What is the capital of the Netherlands?\n",
      "Route: no tool needed\n",
      "Antwort: The capital of the Netherlands is Amsterdam!\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Ihre Anfrage: Where did Bashar flee from Syria in December 2024\n",
      "Route: search tool needed\n",
      "Antwort: Bashar al-Assad fled from Syria in December 2024, boarding a plane in Damascus and flying to an unknown destination. This happened as opposition forces overthrew the government and seized power, ending his 24-year reign.\n",
      "\n",
      "--------------------------------------------------\n",
      "Programm beendet. Auf Wiedersehen!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from groq import Groq\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "# Definiere Modelle\n",
    "ROUTING_MODEL = \"llama3-70b-8192\"\n",
    "TOOL_USE_MODEL = \"llama3-groq-70b-8192-tool-use-preview\"\n",
    "GENERAL_MODEL = \"llama3-70b-8192\"\n",
    "\n",
    "# DuckDuckGo-Tool\n",
    "def search_duckduckgo(query: str):\n",
    "    \"\"\"Verwende das DuckDuckGo-Tool für eine Suchanfrage.\"\"\"\n",
    "    try:\n",
    "        search_tool = DuckDuckGoSearchRun()\n",
    "        result = search_tool.run(query)\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Ein Fehler ist aufgetreten: {str(e)}\"})\n",
    "\n",
    "def route_query(query):\n",
    "    \"\"\"Routing-Logik zur Bestimmung, ob Tools verwendet werden sollen.\"\"\"\n",
    "    routing_prompt = f\"\"\"\n",
    "    Given the following user query, determine if any tools are needed to answer it.\n",
    "    If a search tool is needed, respond with 'TOOL: SEARCH'.\n",
    "    If no tools are needed, respond with 'NO TOOL'.\n",
    "\n",
    "    User query: {query}\n",
    "\n",
    "    Response:\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=ROUTING_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a routing assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": routing_prompt}\n",
    "        ],\n",
    "        max_tokens=20\n",
    "    )\n",
    "    routing_decision = response.choices[0].message.content.strip()\n",
    "    return \"search tool needed\" if \"TOOL: SEARCH\" in routing_decision else \"no tool needed\"\n",
    "\n",
    "def run_with_tool(query):\n",
    "    \"\"\"Führe das DuckDuckGo-Tool aus, falls erforderlich.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a search assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "    tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"search_duckduckgo\",\n",
    "                \"description\": \"Search DuckDuckGo for answers to a query.\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\"type\": \"string\", \"description\": \"The search query.\"},\n",
    "                    },\n",
    "                    \"required\": [\"query\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        model=TOOL_USE_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        max_tokens=4096\n",
    "    )\n",
    "    response_message = response.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    if tool_calls:\n",
    "        messages.append(response_message)\n",
    "        for tool_call in tool_calls:\n",
    "            function_args = json.loads(tool_call.function.arguments)\n",
    "            function_response = search_duckduckgo(function_args.get(\"query\"))\n",
    "            messages.append(\n",
    "                {\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": str(function_response),\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                }\n",
    "            )\n",
    "        second_response = client.chat.completions.create(\n",
    "            model=TOOL_USE_MODEL,\n",
    "            messages=messages\n",
    "        )\n",
    "        return second_response.choices[0].message.content\n",
    "    return response_message.content\n",
    "\n",
    "def run_general(query):\n",
    "    \"\"\"Verwende das allgemeine Modell für eine Anfrage ohne Tool.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=GENERAL_MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def process_query(query):\n",
    "    \"\"\"Verarbeite die Benutzeranfrage und leite sie an das richtige Modell weiter.\"\"\"\n",
    "    route = route_query(query)\n",
    "    if route == \"search tool needed\":\n",
    "        response = run_with_tool(query)\n",
    "    else:\n",
    "        response = run_general(query)\n",
    "    return {\"query\": query, \"route\": route, \"response\": response}\n",
    "\n",
    "# Hauptprogramm mit Eingabefeld\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"### Willkommen beim intelligenten Agenten! ###\")\n",
    "    print(\"Geben Sie 'exit' ein, um das Programm zu beenden.\\n\")\n",
    "\n",
    "    while True:\n",
    "        # Benutzerabfrage eingeben lassen\n",
    "        user_query = input(\"Bitte geben Sie Ihre Anfrage ein: \")\n",
    "        if user_query.lower() == \"exit\":\n",
    "            print(\"Programm beendet. Auf Wiedersehen!\")\n",
    "            break\n",
    "\n",
    "        # Anfrage verarbeiten\n",
    "        result = process_query(user_query)\n",
    "        \n",
    "        # Ausgabe des Ergebnisses\n",
    "        print(f\"\\nIhre Anfrage: {result['query']}\")\n",
    "        print(f\"Route: {result['route']}\")\n",
    "        print(f\"Antwort: {result['response']}\\n\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "#        \"What is the capital of the Netherlands?\",\n",
    " #       \"Where did Bashar flee from Syria in December 2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10530976",
   "metadata": {},
   "source": [
    "## Paralleler Tool Aufruf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efa5612",
   "metadata": {},
   "source": [
    "Wir haben jetzt die Verwendung von Tools gelernt und Beispiele für die Verwendung von Tools in einem Arbeitsgang erstellt. Gehen wir nun einen Schritt weiter und stellen uns einen Arbeitsablauf vor, bei dem mehrere Tools gleichzeitig aufgerufen werden können, was effizientere Antworten ermöglicht.\n",
    "\n",
    "Dieses Konzept wird als parallele Toolnutzung bezeichnet und ist der Schlüssel zum Aufbau von agentenbasierten Arbeitsabläufen, die komplexe Abfragen bewältigen können. \n",
    "\n",
    "Wir werden im nächsten Schritt ein Tool zur Ermittlung der Temperatur und parallel ein Tool zur Ermittlung der Wetterbedingungen ausführen, um eine ganzheitliche Antwort zu erhalten.\n",
    "Anders formuliert: wir implementieren ein zweistufiges Verfahren, bei dem der LLM-Agent Reasoning und Tool-Use kombiniert, um die notwendigen Daten aus mehreren Quellen zu holen und sie abschließend zusammenzufassen. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "094cd266",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "# Groq API Key\n",
    "groq_api_key = \"your_groq_key\"\n",
    "\n",
    "# Initialisiere Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "model = \"llama3-groq-70b-8192-tool-use-preview\"  # Groq-Modell mit Tool-Use-Fähigkeiten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1dc91",
   "metadata": {},
   "source": [
    "Durch die Tool-Definitionen im Code erkennt das Modell:  \n",
    "- Welche Funktionen zur Verfügung stehen.  \n",
    "- Welche Parameter die Tools benötigen, um die Anfrage zu verarbeiten.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fa10812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Wetter-Tools: Diese Funktionen liefern Wetterinformationen zu einem bestimmten Ort.\n",
    "def get_temperature(location: str):\n",
    "    \"\"\"\n",
    "    Liefert die Temperatur für eine gegebene Stadt.\n",
    "    \n",
    "    Parameter:\n",
    "        location (str): Die Stadt, für die die Temperatur abgefragt wird.\n",
    "\n",
    "    Rückgabewert:\n",
    "        int oder str: Temperatur in Celsius oder Fehlermeldung.\n",
    "    \"\"\"\n",
    "    temperatures = {\"Hamburg\": 22, \"Berlin\": 18, \"Stuttgart\": 26, \"Dresden\": 20}\n",
    "    return temperatures.get(location, \"Temperature data not available\")\n",
    "\n",
    "def get_weather_condition(location: str):\n",
    "    \"\"\"\n",
    "    Liefert die Wetterbedingungen für eine gegebene Stadt (mocked/fake Daten).\n",
    "    \n",
    "    Parameter:\n",
    "        location (str): Die Stadt, für die die Wetterbedingungen abgefragt werden.\n",
    "\n",
    "    Rückgabewert:\n",
    "        str: Wetterbedingung (z.B. 'Sunny', 'Rainy') oder Fehlermeldung.\n",
    "    \"\"\"\n",
    "    conditions = {\"Hamburg\": \"Sunny\", \"Berlin\": \"Rainy\", \"Stuttgart\": \"Cloudy\", \"Dresden\": \"Clear\"}\n",
    "    return conditions.get(location, \"Weather condition data not available\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Hamburg and Berlin?\"},\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature\",\n",
    "            \"description\": \"Get the temperature for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_condition\",\n",
    "            \"description\": \"Get the weather condition for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c277b",
   "metadata": {},
   "source": [
    "Die Antwort des ersten API-Aufrufs (client.chat.completions.create) enthält:  \n",
    "- Eine oder mehrere tool_calls, d.h., das Modell schlägt vor, welche Tools (Funktionen) es aufrufen will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcfddcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the initial request\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b09fe2",
   "metadata": {},
   "source": [
    "Jedes Tool wird iterativ aufgerufen.  \n",
    "Die Ergebnisse der Tools werden als Nachricht der Rolle „tool“ zurück an das Modell gegeben.  \n",
    "Beispiel:  \n",
    "- Tool-Aufruf: get_temperature(location='Hamburg') → Ergebnis: 22\n",
    "- Tool-Aufruf: get_weather_condition(location='Hamburg') → Ergebnis: Sunny  \n",
    "\n",
    "Das Modell aggregiert die Tool-Ergebnisse in einer abschließenden Antwort  \n",
    "Nachdem die Tool-Antworten gesammelt und an die Nachrichtenhistorie (messages) angehängt wurden, erfolgt ein zweiter API-Aufruf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d3406919",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "# Process tool calls\n",
    "messages.append(response_message)\n",
    "\n",
    "# Ein Dictionary, das die verfügbaren Funktionen (Tools) mit ihren Namen verknüpft.\n",
    "# Die Schlüssel sind die Namen der Funktionen, die vom Modell aufgerufen werden können.\n",
    "available_functions = {\n",
    "    \"get_temperature\": get_temperature,  # Verknüpft mit der get_temperature-Funktion\n",
    "    \"get_weather_condition\": get_weather_condition,  # Verknüpft mit der get_weather_condition-Funktion\n",
    "}\n",
    "\n",
    "# Iteriere über alle vom Modell aufgerufenen Tools (tool_calls).\n",
    "for tool_call in tool_calls:\n",
    "    # Extrahiere den Namen der Funktion, die das Modell aufrufen möchte.\n",
    "    function_name = tool_call.function.name\n",
    "    \n",
    "    # Hole die passende Funktion aus dem Dictionary der verfügbaren Funktionen.\n",
    "    function_to_call = available_functions[function_name]\n",
    "    \n",
    "    # Lade die Argumente der Funktion aus der API-Antwort (JSON-Format).\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    # Rufe die gewünschte Funktion mit den extrahierten Argumenten auf.\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "        # Füge die Antwort des Tools zur Nachrichtenhistorie hinzu, damit das Modell darauf reagieren kann.\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"tool\",  # Die Rolle zeigt an, dass die Antwort von einem Tool kommt.\n",
    "            \"content\": str(function_response),  # Konvertiere die Tool-Antwort in einen String.\n",
    "            \"tool_call_id\": tool_call.id,  # Verknüpfe die Antwort mit der ID des Tool-Aufrufs.\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Make the final request with tool call results\n",
    "final_response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bc2b",
   "metadata": {},
   "source": [
    "Das Modell hat nun Zugriff auf:  \n",
    "- Die ursprüngliche Benutzeranfrage.\n",
    "- Die Tool-Antworten (z.B. Temperatur und Wetterzustand für Hamburg und Berlin).   \n",
    "Basierend darauf formuliert das Modell eine ganzheitliche Antwort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68b34fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Hamburg is sunny, while in Berlin, it's currently rainy.\n"
     ]
    }
   ],
   "source": [
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65e8337",
   "metadata": {},
   "source": [
    "Anbei der komplette Code in einer Zelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "615bb66a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in Hamburg is sunny, and in Berlin, it's rainy.\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "import json\n",
    "# Groq API Key\n",
    "groq_api_key = \"your_groq_key\"\n",
    "\n",
    "# Initialisiere Groq client\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "model = \"llama3-groq-70b-8192-tool-use-preview\"  # Groq-Modell mit Tool-Use-Fähigkeiten\n",
    "\n",
    "# Definition der Wetter-Tools: Diese Funktionen liefern Wetterinformationen zu einem bestimmten Ort.\n",
    "def get_temperature(location: str):\n",
    "    \"\"\"\n",
    "    Liefert die Temperatur für eine gegebene Stadt.\n",
    "    \n",
    "    Parameter:\n",
    "        location (str): Die Stadt, für die die Temperatur abgefragt wird.\n",
    "\n",
    "    Rückgabewert:\n",
    "        int oder str: Temperatur in Celsius oder Fehlermeldung.\n",
    "    \"\"\"\n",
    "    temperatures = {\"Hamburg\": 22, \"Berlin\": 18, \"Stuttgart\": 26, \"Dresden\": 20}\n",
    "    return temperatures.get(location, \"Temperature data not available\")\n",
    "\n",
    "def get_weather_condition(location: str):\n",
    "    \"\"\"\n",
    "    Liefert die Wetterbedingungen für eine gegebene Stadt (mocked/fake Daten).\n",
    "    \n",
    "    Parameter:\n",
    "        location (str): Die Stadt, für die die Wetterbedingungen abgefragt werden.\n",
    "\n",
    "    Rückgabewert:\n",
    "        str: Wetterbedingung (z.B. 'Sunny', 'Rainy') oder Fehlermeldung.\n",
    "    \"\"\"\n",
    "    conditions = {\"Hamburg\": \"Sunny\", \"Berlin\": \"Rainy\", \"Stuttgart\": \"Cloudy\", \"Dresden\": \"Clear\"}\n",
    "    return conditions.get(location, \"Weather condition data not available\")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful weather assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like in Hamburg and Berlin?\"},\n",
    "]\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_temperature\",\n",
    "            \"description\": \"Get the temperature for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_condition\",\n",
    "            \"description\": \"Get the weather condition for a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "# Make the initial request\n",
    "response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "# Process tool calls\n",
    "messages.append(response_message)\n",
    "\n",
    "# Ein Dictionary, das die verfügbaren Funktionen (Tools) mit ihren Namen verknüpft.\n",
    "# Die Schlüssel sind die Namen der Funktionen, die vom Modell aufgerufen werden können.\n",
    "available_functions = {\n",
    "    \"get_temperature\": get_temperature,  # Verknüpft mit der get_temperature-Funktion\n",
    "    \"get_weather_condition\": get_weather_condition,  # Verknüpft mit der get_weather_condition-Funktion\n",
    "}\n",
    "\n",
    "# Iteriere über alle vom Modell aufgerufenen Tools (tool_calls).\n",
    "for tool_call in tool_calls:\n",
    "    # Extrahiere den Namen der Funktion, die das Modell aufrufen möchte.\n",
    "    function_name = tool_call.function.name\n",
    "    \n",
    "    # Hole die passende Funktion aus dem Dictionary der verfügbaren Funktionen.\n",
    "    function_to_call = available_functions[function_name]\n",
    "    \n",
    "    # Lade die Argumente der Funktion aus der API-Antwort (JSON-Format).\n",
    "    function_args = json.loads(tool_call.function.arguments)\n",
    "    \n",
    "    # Rufe die gewünschte Funktion mit den extrahierten Argumenten auf.\n",
    "    function_response = function_to_call(**function_args)\n",
    "\n",
    "    # Füge die Antwort des Tools zur Nachrichtenhistorie hinzu, damit das Modell darauf reagieren kann.\n",
    "    messages.append(\n",
    "        {\n",
    "            \"role\": \"tool\",  # Die Rolle zeigt an, dass die Antwort von einem Tool kommt.\n",
    "            \"content\": str(function_response),  # Konvertiere die Tool-Antwort in einen String.\n",
    "            \"tool_call_id\": tool_call.id,  # Verknüpfe die Antwort mit der ID des Tool-Aufrufs.\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Make the final request with tool call results\n",
    "final_response = client.chat.completions.create(\n",
    "    model=model, messages=messages, tools=tools, tool_choice=\"auto\", max_tokens=4096\n",
    ")\n",
    "\n",
    "print(final_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d4ed43",
   "metadata": {},
   "source": [
    "### Tool Use with Structured Outputs: Warum Typensicherheit wichtig ist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193e7719",
   "metadata": {},
   "source": [
    "Pydantic stellt sicher, dass die Ausgabe des LLMs einer fest definierten Struktur entspricht. So werden wir gleich erzwingen, dass die Antwort Felder wie location, temperature und condition enthält, wodurch das Fehlerrisiko verringert wird.\n",
    "- Ohne Pydantic könnte der LLM-Output unstrukturiert, fehlerhaft oder unvollständig sein.\n",
    "- Pydantic überprüft, ob die ausgegebenen Daten alle notwendigen Felder haben und ob sie den korrekten Datentypen entsprechen (z.B. int für Temperatur).\n",
    "\n",
    "Pydantic geht über eine reine \"Überwachung\" hinaus, da es die gelieferten Rohdaten aktiv in die benötigte Form bringt:\n",
    "- Wenn die Antwort des LLMs z.B. JSON ist und leicht abweicht (z.B. temperature=\"22\" statt temperature=22), versucht Pydantic, die Daten zu korrigieren.\n",
    "- Es wandelt die Eingabedaten (sofern möglich) in die passenden Python-Datentypen um, z.B. Strings zu Integers oder Dictionaries zu Klasseninstanzen.\n",
    "- Falls die Umwandlung fehlschlägt, wird ein Fehler geworfen, sodass der Entwickler das Problem identifizieren kann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4de00ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor  # Instructor-Bibliothek zur Integration von Pydantic-Modellen mit LLM-Clients\n",
    "from pydantic import BaseModel, Field  # Pydantic wird verwendet, um Datenmodelle zu definieren und zu validieren\n",
    "from groq import Groq  # Groq-Client für LLM-API-Interaktionen\n",
    "import json  # Zum Formatieren der Ausgabe\n",
    "\n",
    "# Definition des Tool-Schemas\n",
    "# Hier wird beschrieben, welche Parameter das Tool akzeptiert und wofür es verwendet wird.\n",
    "tool_schema = {\n",
    "    \"name\": \"get_weather_info\",  # Name des Tools\n",
    "    \"description\": \"Get the weather information for any location.\",  # Beschreibung des Tools\n",
    "    \"parameters\": {  # Definiert die Parameter, die das Tool benötigt\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {  # Der Parameter 'location'\n",
    "                \"type\": \"string\",  # Typ des Parameters: String\n",
    "                \"description\": \"The location for which we want to get the weather information (e.g., Hamburg)\"  # Beschreibung des Parameters\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # Der Parameter 'location' ist verpflichtend\n",
    "    }\n",
    "}\n",
    "\n",
    "# Definition des Pydantic-Modells für den Tool-Aufruf\n",
    "# Dieses Modell stellt sicher, dass die Daten des Tools einer festen Struktur folgen.\n",
    "class ToolCall(BaseModel):\n",
    "    input_text: str = Field(description=\"The user's input text\")  # Eingabetext des Benutzers\n",
    "    tool_name: str = Field(description=\"The name of the tool to call\")  # Name des aufgerufenen Tools\n",
    "    tool_parameters: str = Field(description=\"JSON string of tool parameters\")  # Parameter des Tools als JSON-String"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be7f74",
   "metadata": {},
   "source": [
    "Dieser Code definiert Pydantic-Modelle, um die LLM-Antworten (Tool-Aufrufe und Wetterdaten) zu validieren und strukturiert zu verarbeiten.   \n",
    "Eine Konversation wird initiiert, bei der das LLM feststellt, welche Tools (z.B. `get_weather_info`) aufgerufen werden müssen.   \n",
    "Die Ergebnisse der Tools werden simuliert, formatiert und abschließend in einer strukturierten Ausgabe zusammengefasst und angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae454042",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: What's the weather like in Hamburg?\n",
      "Tool: get_weather_info\n",
      "Parameters: {\"location\": \"Hamburg\"}\n",
      "Response: The temperature in Hamburg is 22°C and it's Sunny.\n",
      "Input: What's the weather like in Berlin?\n",
      "Tool: get_weather_info\n",
      "Parameters: {\"location\": \"Berlin\"}\n",
      "Response: The temperature in Berlin is 18°C and it's Rainy.\n",
      "\n",
      "Formatted Output:\n",
      "1. The temperature in Hamburg is 22°C and it's Sunny.\n",
      "2. The temperature in Berlin is 18°C and it's Rainy.\n"
     ]
    }
   ],
   "source": [
    "# Definition des Pydantic-Modells für die API-Antwort\n",
    "# Enthält eine Liste von Tool-Aufrufen (ToolCall)\n",
    "class ResponseModel(BaseModel):\n",
    "    tool_calls: list[ToolCall]  # Liste von Tool-Aufrufen\n",
    "\n",
    "# Definition des Pydantic-Modells für die formatierten Antworten\n",
    "class WeatherResponse(BaseModel):\n",
    "    location: str = Field(description=\"The name of the location\")\n",
    "    temperature: int = Field(description=\"The temperature in Celsius\")\n",
    "    condition: str = Field(description=\"The weather condition, e.g., sunny or rainy\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"The temperature in {self.location} is {self.temperature}°C and it's {self.condition}.\"\n",
    "\n",
    "# Patch des Groq-Clients mit der Instructor-Bibliothek\n",
    "# Dies sorgt dafür, dass die API-Antworten automatisch in die definierten Pydantic-Modelle umgewandelt werden.\n",
    "client = instructor.from_groq(Groq(api_key=groq_api_key), mode=instructor.Mode.JSON)\n",
    "\n",
    "def run_conversation(user_prompt):\n",
    "    # Erstellen der Nachrichten für die Konversation\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",  # Systemnachricht zur Definition der Assistentenrolle\n",
    "            \"content\": f\"You are an assistant that can use tools. You have access to the following tool: {tool_schema}\"  # Tool-Beschreibung\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",  # Benutzernachricht\n",
    "            \"content\": user_prompt,  # Eingabetext des Benutzers\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Aufruf der Groq-API zur Erstellung einer Antwort\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",  # Verwendetes LLM-Modell\n",
    "        response_model=ResponseModel,  # Pydantic-Modell zur Validierung der Antwort\n",
    "        messages=messages,  # Nachrichtenverlauf\n",
    "        temperature=0.7,  # Temperatur zur Steuerung der Kreativität der Antworten\n",
    "        max_tokens=1000,  # Maximale Anzahl der Tokens in der Antwort\n",
    "    )\n",
    "\n",
    "    return response.tool_calls  # Gibt die Liste der Tool-Aufrufe zurück\n",
    "\n",
    "def call_tool_and_get_response(tool_name, tool_parameters):\n",
    "    # Mock-Funktion zur Simulation der Tool-Ausgabe\n",
    "    # In einer realen Umgebung würde hier ein API-Aufruf oder eine andere Logik implementiert werden.\n",
    "    if tool_name == \"get_weather_info\":\n",
    "        params = json.loads(tool_parameters)  # Parameter als JSON parsen\n",
    "        location = params.get(\"location\", \"Unknown\")\n",
    "        # Beispielwerte für die Wetterdaten\n",
    "        mock_weather = {\n",
    "            \"Hamburg\": WeatherResponse(location=\"Hamburg\", temperature=22, condition=\"Sunny\"),\n",
    "            \"Berlin\": WeatherResponse(location=\"Berlin\", temperature=18, condition=\"Rainy\"),\n",
    "            \"San Francisco\": WeatherResponse(location=\"Kiel\", temperature=20, condition=\"Foggy\")\n",
    "        }\n",
    "        return mock_weather.get(location, f\"No weather data for {location}\")\n",
    "    return \"Tool not implemented.\"\n",
    "\n",
    "# Beispiel zur Verwendung der Funktion\n",
    "user_prompt = \"What's the weather like in Hamburg and Berlin?\"  # Eingabe des Benutzers\n",
    "\n",
    "# Die Funktion wird ausgeführt und die Liste der Tool-Aufrufe wird zurückgegeben\n",
    "tool_calls = run_conversation(user_prompt)\n",
    "\n",
    "# Ausgabe der Tool-Aufrufe und der Tool-Ergebnisse\n",
    "responses = []\n",
    "for call in tool_calls:\n",
    "    print(f\"Input: {call.input_text}\")  # Zeigt den Eingabetext des Benutzers\n",
    "    print(f\"Tool: {call.tool_name}\")  # Zeigt den Namen des aufgerufenen Tools\n",
    "    print(f\"Parameters: {call.tool_parameters}\")  # Zeigt die Parameter des Tools\n",
    "    \n",
    "    # Rufe das Tool auf und erhalte die Ausgabe\n",
    "    tool_response = call_tool_and_get_response(call.tool_name, call.tool_parameters)\n",
    "    print(f\"Response: {tool_response}\")  # Zeigt die Antwort des Tools\n",
    "    responses.append(tool_response)\n",
    "\n",
    "# Ausgabe der formatierten Liste\n",
    "print(\"\\nFormatted Output:\")\n",
    "for idx, response in enumerate(responses, start=1):\n",
    "    print(f\"{idx}. {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658ca6f7",
   "metadata": {},
   "source": [
    "Anbei der komplette Code in einer Zelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47021af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Hamburg weather\n",
      "Tool: get_weather_info\n",
      "Parameters: {\"location\": \"Hamburg\"}\n",
      "Response: The temperature in Hamburg is 22°C and it's Sunny.\n",
      "Input: Berlin weather\n",
      "Tool: get_weather_info\n",
      "Parameters: {\"location\": \"Berlin\"}\n",
      "Response: The temperature in Berlin is 18°C and it's Rainy.\n",
      "\n",
      "Formatted Output:\n",
      "1. The temperature in Hamburg is 22°C and it's Sunny.\n",
      "2. The temperature in Berlin is 18°C and it's Rainy.\n"
     ]
    }
   ],
   "source": [
    "import instructor  # Instructor-Bibliothek zur Integration von Pydantic-Modellen mit LLM-Clients\n",
    "from pydantic import BaseModel, Field  # Pydantic wird verwendet, um Datenmodelle zu definieren und zu validieren\n",
    "from groq import Groq  # Groq-Client für LLM-API-Interaktionen\n",
    "import json  # Zum Formatieren der Ausgabe\n",
    "\n",
    "# Definition des Tool-Schemas\n",
    "# Hier wird beschrieben, welche Parameter das Tool akzeptiert und wofür es verwendet wird.\n",
    "tool_schema = {\n",
    "    \"name\": \"get_weather_info\",  # Name des Tools\n",
    "    \"description\": \"Get the weather information for any location.\",  # Beschreibung des Tools\n",
    "    \"parameters\": {  # Definiert die Parameter, die das Tool benötigt\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {  # Der Parameter 'location'\n",
    "                \"type\": \"string\",  # Typ des Parameters: String\n",
    "                \"description\": \"The location for which we want to get the weather information (e.g., Hamburg)\"  # Beschreibung des Parameters\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"location\"]  # Der Parameter 'location' ist verpflichtend\n",
    "    }\n",
    "}\n",
    "\n",
    "# Definition des Pydantic-Modells für den Tool-Aufruf\n",
    "# Dieses Modell stellt sicher, dass die Daten des Tools einer festen Struktur folgen.\n",
    "class ToolCall(BaseModel):\n",
    "    input_text: str = Field(description=\"The user's input text\")  # Eingabetext des Benutzers\n",
    "    tool_name: str = Field(description=\"The name of the tool to call\")  # Name des aufgerufenen Tools\n",
    "    tool_parameters: str = Field(description=\"JSON string of tool parameters\")  # Parameter des Tools als JSON-String\n",
    "\n",
    "# Definition des Pydantic-Modells für die API-Antwort\n",
    "# Enthält eine Liste von Tool-Aufrufen (ToolCall)\n",
    "class ResponseModel(BaseModel):\n",
    "    tool_calls: list[ToolCall]  # Liste von Tool-Aufrufen\n",
    "\n",
    "# Definition des Pydantic-Modells für die formatierten Antworten\n",
    "class WeatherResponse(BaseModel):\n",
    "    location: str = Field(description=\"The name of the location\")\n",
    "    temperature: int = Field(description=\"The temperature in Celsius\")\n",
    "    condition: str = Field(description=\"The weather condition, e.g., sunny or rainy\")\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"The temperature in {self.location} is {self.temperature}°C and it's {self.condition}.\"\n",
    "\n",
    "# Patch des Groq-Clients mit der Instructor-Bibliothek\n",
    "# Dies sorgt dafür, dass die API-Antworten automatisch in die definierten Pydantic-Modelle umgewandelt werden.\n",
    "client = instructor.from_groq(Groq(api_key=groq_api_key), mode=instructor.Mode.JSON)\n",
    "\n",
    "def run_conversation(user_prompt):\n",
    "    # Erstellen der Nachrichten für die Konversation\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",  # Systemnachricht zur Definition der Assistentenrolle\n",
    "            \"content\": f\"You are an assistant that can use tools. You have access to the following tool: {tool_schema}\"  # Tool-Beschreibung\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",  # Benutzernachricht\n",
    "            \"content\": user_prompt,  # Eingabetext des Benutzers\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Aufruf der Groq-API zur Erstellung einer Antwort\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"llama-3.1-70b-versatile\",  # Verwendetes LLM-Modell\n",
    "        response_model=ResponseModel,  # Pydantic-Modell zur Validierung der Antwort\n",
    "        messages=messages,  # Nachrichtenverlauf\n",
    "        temperature=0.7,  # Temperatur zur Steuerung der Kreativität der Antworten\n",
    "        max_tokens=1000,  # Maximale Anzahl der Tokens in der Antwort\n",
    "    )\n",
    "\n",
    "    return response.tool_calls  # Gibt die Liste der Tool-Aufrufe zurück\n",
    "\n",
    "def call_tool_and_get_response(tool_name, tool_parameters):\n",
    "    # Mock-Funktion zur Simulation der Tool-Ausgabe\n",
    "    # In einer realen Umgebung würde hier ein API-Aufruf oder eine andere Logik implementiert werden.\n",
    "    if tool_name == \"get_weather_info\":\n",
    "        params = json.loads(tool_parameters)  # Parameter als JSON parsen\n",
    "        location = params.get(\"location\", \"Unknown\")\n",
    "        # Beispielwerte für die Wetterdaten\n",
    "        mock_weather = {\n",
    "            \"Hamburg\": WeatherResponse(location=\"Hamburg\", temperature=22, condition=\"Sunny\"),\n",
    "            \"Berlin\": WeatherResponse(location=\"Berlin\", temperature=18, condition=\"Rainy\"),\n",
    "            \"San Francisco\": WeatherResponse(location=\"Kiel\", temperature=20, condition=\"Foggy\")\n",
    "        }\n",
    "        return mock_weather.get(location, f\"No weather data for {location}\")\n",
    "    return \"Tool not implemented.\"\n",
    "\n",
    "# Beispiel zur Verwendung der Funktion\n",
    "user_prompt = \"What's the weather like in Hamburg and Berlin?\"  # Eingabe des Benutzers\n",
    "\n",
    "# Die Funktion wird ausgeführt und die Liste der Tool-Aufrufe wird zurückgegeben\n",
    "tool_calls = run_conversation(user_prompt)\n",
    "\n",
    "# Ausgabe der Tool-Aufrufe und der Tool-Ergebnisse\n",
    "responses = []\n",
    "for call in tool_calls:\n",
    "    print(f\"Input: {call.input_text}\")  # Zeigt den Eingabetext des Benutzers\n",
    "    print(f\"Tool: {call.tool_name}\")  # Zeigt den Namen des aufgerufenen Tools\n",
    "    print(f\"Parameters: {call.tool_parameters}\")  # Zeigt die Parameter des Tools\n",
    "    \n",
    "    # Rufe das Tool auf und erhalte die Ausgabe\n",
    "    tool_response = call_tool_and_get_response(call.tool_name, call.tool_parameters)\n",
    "    print(f\"Response: {tool_response}\")  # Zeigt die Antwort des Tools\n",
    "    responses.append(tool_response)\n",
    "\n",
    "# Ausgabe der formatierten Liste\n",
    "print(\"\\nFormatted Output:\")\n",
    "for idx, response in enumerate(responses, start=1):\n",
    "    print(f\"{idx}. {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f1d736",
   "metadata": {},
   "source": [
    "# Langchain und Langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ec51dd",
   "metadata": {},
   "source": [
    "Agenten Frameworks sind praktisch, um von zustandslosen Interaktionen mit LLMs zu einer zustandsorientierten zu kommen. Zwei sehr beliebte Frameworks sind LangGraph und LangChain.  \n",
    "Mithilfe von LangChain's ChatPromptTemplate wird eine Eingabeaufforderung definiert, die dynamisch Systemnachrichten und Toolnamen integriert. Platzhalter wie MessagesPlaceholder ermöglichen das Einfügen eines Nachrichtenverlaufs, wodurch der Agent flexibel auf Konversationen reagieren kann. LangGraph bietet die Struktur für den Fluss der Zustände (z. B. START und END) und ermöglicht, dass der Agent Tools verwendet, die über llm.bind_tools eingebunden werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6d0fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    "    HumanMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"\n",
    "    Erstellt einen Agenten.\n",
    "    \n",
    "    Parameter:\n",
    "    - llm: Das Sprachmodell (Large Language Model), das verwendet wird.\n",
    "    - tools: Eine Liste von Tools, die der Agent verwenden kann, um Aufgaben zu lösen.\n",
    "    - system_message: Eine zusätzliche agentenspezifische Systemnachricht, die Anweisungen enthält.\n",
    "    \"\"\"\n",
    "    # Definiere eine Eingabeaufforderung (Prompt), die die Rolle des Agenten beschreibt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            # Platzhalter für Nachrichtenverlauf (dynamische Einbindung von Nachrichten)\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Die Systemnachricht wird als Teil des Prompts gesetzt\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "\n",
    "    # Fügt die Namen der verfügbaren Tools in die Eingabeaufforderung ein\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "\n",
    "    # Verbindet das LLM mit den Tools und gib das konfigurierte Agentenobjekt zurück\n",
    "    return prompt | llm.bind_tools(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4825a8",
   "metadata": {},
   "source": [
    "Wir implementieren eine Suchfunktion, die mithilfe des DuckDuckGoSearchRun-Tools von LangChain Anfragen ausführt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9790e90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_9112\\498103034.py:54: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  search_result = search(query)            # Führe die Suche aus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"result\": \"In 2024, generative AI might actually become useful for the regular, non-tech person, and we are going to see more people tinkering with a million little AI models. ... Get the latest updates from ... The year 2024 has been marked by notable advancements in artificial intelligence (AI) that promise to reshape various aspects of technology, industry, and society. Among these breakthroughs are critical developments from tech giants and innovative approaches influencing the global AI landscape. Here, we explore seven pivotal AI developments, including historic regulatory frameworks and Nobel Prize-winning breakthroughs, that are reshaping business and society. Custom enterprise models, open source AI, multimodal -- learn about the top AI and machine learning trends for 2024 and how they promise to transform the industry. After the launch of ChatGPT in November 2022, 2023 marked a turning point in artificial intelligence. In 2024, artificial intelligence (AI) will continue to revolutionize various sectors globally. Research institutions, companies, and governments are driving advancements in AI technologies, contributing to significant breakthroughs and applications. This article provides an overview of the leading A\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import json\n",
    "from langchain.tools import DuckDuckGoSearchRun\n",
    "from langchain_core.tools import tool\n",
    "from groq import Groq\n",
    "\n",
    "# API-Schlüssel für Groq, ersetzt durch deinen tatsächlichen Schlüssel\n",
    "groq_api_key = \"your_groq_key\"\n",
    "\n",
    "# Initialisiere den Groq-Client mit dem API-Schlüssel\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Initialisiere das Groq-Modell (ChatGroq) mit spezifischen Parametern\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",  # Wähle ein Modell: z. B. \"llama3-8b-8192\" oder \"mixtral-8x7b-32768\"\n",
    "    temperature=0,               # Temperatur für die Ausgabe (niedrig = deterministischer)\n",
    "    max_tokens=None,             # Maximale Anzahl von Tokens (None = unbegrenzt)\n",
    "    timeout=None,                # Timeout für API-Aufrufe\n",
    "    max_retries=2,               # Maximale Anzahl von Wiederholungen bei Fehlern\n",
    "    api_key=groq_api_key         # API-Schlüssel für das Groq-Modell\n",
    ")\n",
    "\n",
    "# Definiere die Suchfunktion mit DuckDuckGo\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Führt eine Suche mit DuckDuckGo über das LangChain DuckDuckGoSearchRun-Tool aus.\n",
    "    \n",
    "    Parameter:\n",
    "        query (str): Die Suchanfrage, die an DuckDuckGo gesendet wird.\n",
    "        \n",
    "    Rückgabewert:\n",
    "        str: Ein JSON-String, der entweder die Suchergebnisse oder eine Fehlermeldung enthält.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Erstelle eine Instanz des DuckDuckGo-Suchtools\n",
    "        search_tool = DuckDuckGoSearchRun()\n",
    "        \n",
    "        # Führe die Suche mit der übergebenen Anfrage aus\n",
    "        result = search_tool.run(query)\n",
    "        \n",
    "        # Rückgabe des Ergebnisses als JSON-String\n",
    "        return json.dumps({\"result\": result})\n",
    "    except Exception as e:\n",
    "        # Im Fehlerfall wird eine Fehlermeldung als JSON zurückgegeben\n",
    "        return json.dumps({\"error\": f\"Ein Fehler ist aufgetreten: {str(e)}\"})\n",
    "\n",
    "# Füge die Suchfunktion zur Werkzeugliste hinzu\n",
    "tools = [search]\n",
    "\n",
    "# Beispielverwendung der Suchfunktion\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Latest advancements in AI 2024\"  # Die Suchanfrage\n",
    "    search_result = search(query)            # Führe die Suche aus\n",
    "    print(search_result)                     # Drucke das Suchergebnis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ede10c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install exa_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49bac32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_22964\\3763697554.py:77: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  search_result = search(query)  # Führe die Suche aus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"title\": \"Thousands of AI Authors on the Future of AI\", \"link\": \"http://arxiv.org/abs/2401.02843\", \"snippet\": \"View PDF HTML (experimental) Abstract: In the largest survey of its kind, 2,778 researchers who had published in top-tier artificial intelligence (AI) venues gave predictions on the pace of AI progress and the nature and impacts of advanced AI systems The aggregate forecasts give at least a 50% chance of AI systems achieving several milestones by 2028, including autonomously constructing a payment processing site from scratch, creating a song indistinguishable from a new song by a popular musician, and autonomously downloading and fine-tuning a large language model. If science continues undisrupted, the chance of unaided machines outperforming humans in every possible task was estimated at 10% by 2027, and 50% by 2047. The latter estimate is 13 years earlier than that reached in a similar survey we conducted only one year earlier [Grace et al., 2022]. However, the chance of all human occupations becoming fully automatable was forecast to reach 10% by 2037, and 50% as late as 2116 (compared to 2164 in the 2022 survey). Most respondents expressed substantial uncertainty about the long-term value of AI progress: While 68.3% thought good outcomes from superhuman AI are more likely than bad, of these net optimists 48% gave at least a 5% chance of extremely bad outcomes such as human extinction, and 59% of net pessimists gave 5% or more to extremely good outcomes. Between 38% and 51% of respondents gave at least a 10% chance to advanced AI leading to outcomes as bad as human extinction. More than half suggested that \\\"substantial\\\" or \\\"extreme\\\" concern is warranted about six different AI-related scenarios, including misinformation, authoritarian control, and inequality. There was disagreement about whether faster or slower AI progress would be better for the future of humanity. However, there was broad agreement that research aimed at minimizing potential risks from AI systems ought to be prioritized more. Submission history From: Julia Fabienne Sandk\\u00fchler [ view email ] [v1] Fri, 5 Jan 2024 14:53:09 UTC (4,906 KB) [v2] Tue, 30 Apr 2024 18:15:42 UTC (4,907 KB)\", \"score\": 0.17473313212394714, \"publish\": \"2024-01-05T00:00:00.000Z\"}, {\"title\": \"Artificial Intelligence Index Report 2024\", \"link\": \"https://arxiv.org/abs/2405.19522\", \"snippet\": \"https://arxiv.org/abs/2405.19522 Artificial Intelligence Index Report 2024 2024-05-29T00:00:00Z [Submitted on 29 May 2024] Authors: Nestor Maslej , Loredana Fattorini , Raymond Perrault , Vanessa Parli , Anka Reuel , Erik Brynjolfsson , John Etchemendy , Katrina Ligett , Terah Lyons , James Manyika , Juan Carlos Niebles , Yoav Shoham , Russell Wald , Jack Clark View PDF Abstract: The 2024 Index is our most comprehensive to date and arrives at an important moment when AI&#39;s influence on society has never been more pronounced. This year, we have broadened our scope to more extensively cover essential trends such as technical advancements in AI, public perceptions of the technology, and the geopolitical dynamics surrounding its development. Featuring more original data than ever before, this edition introduces new estimates on AI training costs, detailed analyses of the responsible AI landscape, and an entirely new chapter dedicated to AI&#39;s impact on science and medicine. The AI Index report tracks, collates, distills, and visualizes data related to artificial intelligence (AI). Our mission is to provide unbiased, rigorously vetted, broadly sourced data in order for policymakers, researchers, executives, journalists, and the general public to develop a more thorough and nuanced understanding of the complex field of AI. The AI Index is recognized globally as one of the most credible and authoritative sources for data and insights on artificial intelligence. Previous editions have been cited in major newspapers, including the The New York Times, Bloomberg, and The Guardian, have amassed hundreds of academic citations, and been referenced by high-level policymakers in the United States, the United Kingdom, and the European Union, among other places. This year&#39;s edition surpasses all previous ones in size, scale, and scope, reflecting the growing significance that AI is coming to hold in all of our lives. Submission history From: Nestor Maslej [ view email ] [v1] Wed, 29 May 2024 20:59:57 UTC (43,091 KB)\", \"score\": 0.17413602769374847, \"publish\": \"2024-05-29T00:00:00.000Z\"}, {\"title\": \"\", \"link\": \"https://arxiv.org/pdf/2405.10313\", \"snippet\": \"How Far Are We From AGI? Tao Feng1\\u2217, Chuanyang Jin2\\u2217\\u2020, Jingyu Liu3\\u2217\\u2020, Kunlun Zhu1\\u2217 Haoqin Tu4\\u2020, Zirui Cheng1, Guanyu Lin5\\u2020, Jiaxuan You1\\u2021 {taofeng2, kunlunz2, jiaxuan}@illinois.edu 1University of Illinois Urbana-Champaign 2 Johns Hopkins University 3University of Chicago 4University of California, Santa Cruz 5Carnegie Mellon University Abstract The evolution of artificial intelligence (AI) has profoundly impacted human society, driv\\u0002ing significant advancements in multiple sectors. Yet, the escalating demands on AI have highlighted the limitations of AI\\u2019s current offerings, catalyzing a movement towards Arti\\u0002ficial General Intelligence (AGI). AGI, distinguished by its ability to execute diverse real\\u0002world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing works have summarized specific recent advancements of AI, they lack a comprehensive discussion of AGI\\u2019s definitions, goals, and developmental trajectories. Different from existing survey papers, this paper delves into the pivotal questions of our proximity to AGI and the strategies necessary for its realization through extensive surveys, discussions, and original perspectives. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmo\\u0002nize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status-quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI1. \\u2217Equal contribution. In alphabetical order. \\u2020Work done as an intern at UIUC. \\u2021Corresponding author 1Project page: https://github.com/ulab-uiuc/AGI-survey. We highly welcome pull requests from the community. 1 arXiv:2405.10313v1 [cs.AI] 16 May 2024 Contents 1 Introduction 4 2 AGI Internal: Unveiling the Mind of AGI 6 2.1 AI Perception . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.2 AI Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3 AI Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.4 AI Metacognition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 3 AGI Interface: Connecting the World with AGI 17 3.1 AI Interfaces to Digital World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.2 AI Interfaces to Physical World . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3.3 AI Interfaces to Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.3.1 AI Interface to Other AI agents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 3.3.2 AI Interfaces to Humans . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 4 AGI Systems: Implementing the Mechanism of AGI 25 4.1 System Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 4.2 Scalable Model Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 4.3 Large-scale Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 4.4 Inference Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 4.5 Cost and Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 4.6 Computing Platforms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 4.7 The Future of AGI Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 5 AGI Alignment: Ensuring AGI Meets Various Needs 37 5.1 Expectations of AGI Alignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 5.2 Current Alignment Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.3 How to approach AGI Alignments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 6 AGI Roadmap: Responsibly Approaching AGI 44 6.1 AGI Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 6.2 AGI Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 6.2.1 Expectations for AGI Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 6.2.2 Current Evaluations and Their Limitations . . . . . . . . . . . . . . . . . . . . . . . . 50 6.3 How to Get to the Next AGI Level . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 6.4 \\u201cHow Far Are We from AGI\\u201d Workshop Discussions . . . . . . . . . . . . . . . . . . . . . . . 54 6.5 Further Considerations during AGI Development . . . . . . . . . . . . . . . . . . . . . . . . . 59 2 7 Case Studies: A Bright Future with AGI 63 7.1 AI for Science Discovery and Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64 7.2 Generative Visual Intelligence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66 7.3 World Models for AGI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67 7.4 Decentralized AI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68 7.5 AI for Coding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 7.6 Embodied AI: AI for Robotics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 7.7 Human-AI Collaboration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73 8 Conclusion 74 3\", \"score\": 0.16220347583293915, \"publish\": \"2024-05-16T00:00:00.000Z\"}, {\"title\": \"AI in Action: Accelerating Progress Towards the Sustainable Development Goals\", \"link\": \"https://arxiv.org/abs/2407.02711\", \"snippet\": \"Authors: Brigitte Hoyer Gosselink , Kate Brandt , Marian Croak , Karen DeSalvo , Ben Gomes , Lila Ibrahim , Maggie Johnson , Yossi Matias , Ruth Porat , Kent Walker , James Manyika View PDF Abstract: Advances in Artificial Intelligence (AI) are helping tackle a growing number of societal challenges, demonstrating technology's increasing capability to address complex issues, including those outlined in the United Nations (UN) Sustainable Development Goals (SDGs). Despite global efforts, 80 percent of SDG targets have deviated, stalled, or regressed, and only 15 percent are on track as of 2023, illustrating the urgency of accelerating efforts to meet the goals by 2030. We draw on Google's internal and collaborative research, technical work, and social impact initiatives to show AI's potential to accelerate action on the SDGs and make substantive progress to help address humanity's most pressing challenges. The paper highlights AI capabilities (including computer vision, generative AI, natural language processing, and multimodal AI) and showcases how AI is altering how we approach problem-solving across all 17 SDGs through use cases, with a spotlight on AI-powered innovation in health, education, and climate. We then offer insights on AI development and deployment to drive bold and responsible innovation, enhance impact, close the accessibility gap, and ensure that everyone, everywhere, can benefit from AI. Submission history From: Brigitte Gosselink [ view email ] [v1] Tue, 2 Jul 2024 23:25:27 UTC (337 KB)\", \"score\": 0.160678431391716, \"publish\": \"2024-07-02T00:00:00.000Z\"}, {\"title\": \"Brain-inspired Artificial Intelligence: A Comprehensive Review\", \"link\": \"https://arxiv.org/abs/2408.14811\", \"snippet\": \"View PDF HTML (experimental) Current artificial intelligence (AI) models often focus on enhancing performance through meticulous parameter tuning and optimization techniques. However, the fundamental design principles behind these models receive comparatively less attention, which can limit our understanding of their potential and constraints. This comprehensive review explores the diverse design inspirations that have shaped modern AI models, i.e., brain-inspired artificial intelligence (BIAI). We present a classification framework that categorizes BIAI approaches into physical structure-inspired and human behavior-inspired models. We also examine the real-world applications where different BIAI models excel, highlighting their practical benefits and deployment challenges. By delving into these areas, we provide new insights and propose future research directions to drive innovation and address current gaps in the field. This review offers researchers and practitioners a comprehensive overview of the BIAI landscape, helping them harness its potential and expedite advancements in AI development. Submission history From: Jing Ren [ view email ] [v1] Tue, 27 Aug 2024 06:49:50 UTC (442 KB)\", \"score\": 0.16020497679710388, \"publish\": \"2024-08-27T00:00:00.000Z\"}]\n"
     ]
    }
   ],
   "source": [
    "# https://exa.ai/\n",
    "from exa_py import Exa  # Import der Exa SDK-Bibliothek\n",
    "from langchain_groq import ChatGroq  # Import für die Verwendung von Groq-Modellen\n",
    "import json  # Modul zur Arbeit mit JSON-Daten\n",
    "from groq import Groq  # Import des Groq-Clients\n",
    "from typing import Literal  # Import zur Typdefinition\n",
    "from langchain_core.tools import tool  # Import der Tool-Dekoration aus LangChain\n",
    "\n",
    "# Setze die API-Schlüssel\n",
    "groq_api_key = \"your_groq_key\"  # Groq API-Schlüssel\n",
    "exa_api_key = \"your_exa_key\"  # Exa API-Schlüssel (ersetzen mit tatsächlichem Schlüssel)\n",
    "\n",
    "# Initialisiere den Groq-Client mit dem API-Schlüssel\n",
    "client = Groq(api_key=groq_api_key)\n",
    "\n",
    "# Initialisiere das ChatGroq-Modell mit spezifischen Parametern\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",  # Wähle das zu verwendende Modell\n",
    "    temperature=0,  # Temperatur für deterministische Ausgaben\n",
    "    max_tokens=400,  # Unbegrenzte Anzahl von Tokens (optional begrenzen)\n",
    "    timeout=None,  # Kein spezifisches Timeout festgelegt\n",
    "    max_retries=2,  # Maximale Anzahl der Wiederholungen bei API-Fehlern\n",
    "    api_key=groq_api_key,  # API-Schlüssel für das Groq-Modell\n",
    ")\n",
    "\n",
    "# Initialisiere den Exa-Client mit dem API-Schlüssel\n",
    "exa = Exa(api_key=exa_api_key)\n",
    "\n",
    "# Definiere die Suchfunktion\n",
    "@tool\n",
    "def search(query: str) -> list:\n",
    "    \"\"\"\n",
    "    Führt eine Suche mit der Exa SDK gegen die gegebene Anfrage aus.\n",
    "\n",
    "    Parameter:\n",
    "        query (str): Die Suchanfrage.\n",
    "    Rückgabewert:\n",
    "        list: Eine Liste der Suchergebnisse im JSON-Format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Führe die Suche mit der Exa API aus\n",
    "        result = exa.search_and_contents(\n",
    "            query=query,  # Die Suchanfrage\n",
    "            type=\"neural\",  # Verwende neuronale Suche\n",
    "            include_domains=[\"arxiv.org\", \"bing.com\", \"google.com\"],  # Eingeschränkte Domains\n",
    "            start_published_date=\"2023-12-31\",  # Startdatum für die Ergebnisse\n",
    "            end_published_date=\"2024-9-12\",  # Enddatum für die Ergebnisse\n",
    "            use_autoprompt=True,  # Automatische Generierung von Prompts aktivieren\n",
    "            num_results=5,  # Maximale Anzahl der Ergebnisse\n",
    "            text=True,  # Ergebnisse als Textformat\n",
    "        )\n",
    "\n",
    "        # Formatiere die Ergebnisse\n",
    "        output = []\n",
    "        for item in result.results:  # Iteriere über die Ergebnisse\n",
    "            output.append({\n",
    "                \"title\": item.title,  # Titel des Ergebnisses\n",
    "                \"link\": item.url,  # URL des Ergebnisses\n",
    "                \"snippet\": item.text,  # Textausschnitt des Ergebnisses\n",
    "                \"score\": item.score,  # Relevanzbewertung\n",
    "                \"publish\": item.published_date,  # Veröffentlichungsdatum\n",
    "            })\n",
    "\n",
    "        # Rückgabe der Ergebnisse als JSON\n",
    "        return json.dumps(output)\n",
    "    except Exception as e:\n",
    "        # Fehlerbehandlung: Gib den Fehler als JSON zurück\n",
    "        print(f\"Error during search: {e}\")\n",
    "        return json.dumps({\"error\": str(e)})\n",
    "\n",
    "# Füge die Suchfunktion zur Werkzeugliste hinzu\n",
    "tools = [search]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Latest advancements in AI 2024\"  \n",
    "    search_result = search(query)  # Führe die Suche aus\n",
    "    print(search_result)  # Gib die Suchergebnisse aus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e53cc80",
   "metadata": {},
   "source": [
    "# Definition des Zustands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e46d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator  # Import für die Verwendung von Operatoren, z. B. zur Verkettung von Listen\n",
    "from typing import Annotated, Sequence  # Typannotationen zur Definition von Datentypen\n",
    "from typing_extensions import TypedDict  # TypedDict zur Erstellung von typisierten Dictionaries\n",
    "\n",
    "from langchain_groq import ChatGroq  # Import des ChatGroq-Modells für den Einsatz von KI-Agenten\n",
    "\n",
    "# Diese Klasse definiert das Objekt, das zwischen den Knoten im Graphen ausgetauscht wird.\n",
    "# Jeder Knoten im Graph repräsentiert einen Agenten oder ein Tool, das Teil des Workflows ist.\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"\n",
    "    AgentState beschreibt den Zustand, der zwischen verschiedenen Knoten eines Graphen \n",
    "    weitergegeben wird. Dies umfasst Nachrichten und den Absender der Nachricht.\n",
    "    \"\"\"\n",
    "    # Die Nachrichten, die als Sequenz zwischen Knoten weitergegeben werden.\n",
    "    # Annotated erlaubt die Angabe zusätzlicher Informationen (z. B. Operator zur Verkettung).\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Der Absender der Nachrichten (z. B. der Name des Agenten oder Tools, das die Nachricht gesendet hat)\n",
    "    sender: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1091a4",
   "metadata": {},
   "source": [
    "Sobald das definiert ist, können wir unsere Agentenknoten wie unten gezeigt zusammenstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e124add",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools  # Import für die Verwendung von functools.partial, um Funktionen teilweise zu binden\n",
    "\n",
    "from langchain_core.messages import AIMessage  # Import für die AIMessage-Klasse, um KI-generierte Nachrichten zu erstellen\n",
    "\n",
    "# Hilfsfunktion, um einen Knoten für einen bestimmten Agenten zu erstellen\n",
    "def agent_node(state, agent, name):\n",
    "    \"\"\"\n",
    "    Erstellt einen Knoten für den angegebenen Agenten und verarbeitet den aktuellen Zustand.\n",
    "    \n",
    "    Parameter:\n",
    "        state: Der aktuelle Zustand, der vom Agenten verarbeitet wird.\n",
    "        agent: Der Agent, der den Zustand verarbeitet.\n",
    "        name: Der Name des Agenten (z. B. für die Sender-Identifikation).\n",
    "    \n",
    "    Rückgabewert:\n",
    "        dict: Enthält die generierten Nachrichten und den Namen des Senders.\n",
    "    \"\"\"\n",
    "    # Agent wird ausgeführt und gibt ein Ergebnis zurück\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # Konvertiere die Ausgabe des Agenten in ein passendes Format\n",
    "    if isinstance(result, ToolMessage):\n",
    "        pass  # ToolMessage wird unverändert übernommen (kann hier erweitert werden, falls nötig)\n",
    "    else:\n",
    "        # Konvertiere die Ausgabe in ein AIMessage-Objekt und füge den Sendernamen hinzu\n",
    "        result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n",
    "    \n",
    "    # Rückgabe des neuen Zustands mit Nachrichten und Sender\n",
    "    return {\n",
    "        \"messages\": [result],  # Die generierte Nachricht wird angefügt\n",
    "        # Da der Workflow streng definiert ist, wird der Sender festgehalten,\n",
    "        # um zu wissen, wer als nächstes die Nachricht verarbeiten soll.\n",
    "        \"sender\": name,\n",
    "    }\n",
    "\n",
    "# Spezifikation des Modells, das von den Agenten verwendet wird\n",
    "llm = ChatGroq(\n",
    "    model=\"mixtral-8x7b-32768\",  \n",
    "    api_key=groq_api_key,        \n",
    ")\n",
    "\n",
    "# Forschungs-Agent und zugehöriger Knoten\n",
    "research_agent = create_agent(\n",
    "    llm,  # Das Modell, das der Agent verwendet\n",
    "    tools,  # Die verfügbaren Tools für den Agenten\n",
    "    system_message=\"You should provide accurate data for the summary_agent to use.\",  # Systemnachricht, die die Rolle beschreibt\n",
    ")\n",
    "# Erstelle einen Knoten für den Forschungs-Agenten\n",
    "research_node = functools.partial(agent_node, agent=research_agent, name=\"Researcher\")\n",
    "\n",
    "# Zusammenfassungs-Agent (Summary Agent) und zugehöriger Knoten\n",
    "summary_agent = create_agent(\n",
    "    llm,  # Das Modell, das der Agent verwendet\n",
    "    tools,  # Die verfügbaren Tools für den Agenten\n",
    "    system_message=\"Any charts you display will be visible by the user.\",  # Systemnachricht für die Rolle des Agenten\n",
    ")\n",
    "# Erstelle einen Knoten für den Zusammenfassungs-Agenten\n",
    "chart_node = functools.partial(agent_node, agent=summary_agent, name=\"summary_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefbabb4",
   "metadata": {},
   "source": [
    "Wir definieren zwei Bereiche: den zusammenfassenden Agenten und den Forschungsagenten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a53d2f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nToolNode ist ein vorgefertigter Knoten aus LangGraph, der die Integration von Tools ermöglicht.\\nDieser Knoten kann im Graph verwendet werden, um die definierten Werkzeuge auszuführen.\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.prebuilt import ToolNode  # Import von ToolNode, um Werkzeuge in den Graph zu integrieren\n",
    "\n",
    "\n",
    "tools = [search]  # Hier wird die zuvor definierte Suchfunktion `search` als Werkzeug hinzugefügt\n",
    "\n",
    "# Erstelle einen Tool-Knoten für den Workflow\n",
    "tool_node = ToolNode(tools)\n",
    "\"\"\"\n",
    "ToolNode ist ein vorgefertigter Knoten aus LangGraph, der die Integration von Tools ermöglicht.\n",
    "Dieser Knoten kann im Graph verwendet werden, um die definierten Werkzeuge auszuführen.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1aec2d",
   "metadata": {},
   "source": [
    "Der nächste Code ist die Edge Logic, mit der wir auf der Grundlage der Ergebnisse der Agenten entscheiden wollen, was zu tun ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f425d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jeder Agent kann entscheiden, den Prozess zu beenden\n",
    "from typing import Literal  # Import zur Definition von Literal-Typen für Rückgabewerte\n",
    "\n",
    "\n",
    "def router(state) -> Literal[\"call_tool\", \"__end__\", \"continue\"]:\n",
    "    \"\"\"\n",
    "    Router-Funktion, die den nächsten Schritt im Workflow bestimmt.\n",
    "    \n",
    "    Parameter:\n",
    "        state (dict): Der aktuelle Zustand, der Nachrichten und weitere Informationen enthält.\n",
    "    \n",
    "    Rückgabewert:\n",
    "        Literal: Ein String, der den nächsten Schritt angibt:\n",
    "        - \"call_tool\": Ein Werkzeug soll aufgerufen werden.\n",
    "        - \"__end__\": Der Workflow wird beendet.\n",
    "        - \"continue\": Der Workflow wird fortgesetzt.\n",
    "    \"\"\"\n",
    "    # Nachrichten aus dem aktuellen Zustand abrufen\n",
    "    messages = state[\"messages\"]\n",
    "    # Die letzte Nachricht im Nachrichtenverlauf\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # Prüfen, ob die letzte Nachricht ein Werkzeug aufrufen möchte\n",
    "    if last_message.tool_calls:\n",
    "        # Der vorherige Agent möchte ein Werkzeug ausführen\n",
    "        return \"call_tool\"\n",
    "    \n",
    "    # Prüfen, ob \"FINAL ANSWER\" in der Nachricht vorhanden ist\n",
    "    if \"FINAL ANSWER\" in last_message.content:\n",
    "        # Ein Agent hat entschieden, dass die Arbeit abgeschlossen ist\n",
    "        return \"__end__\"\n",
    "    \n",
    "    # Standardmäßig wird der Workflow fortgesetzt\n",
    "    return \"continue\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb57cb",
   "metadata": {},
   "source": [
    "Als Nächstes definiert unser Code den StateGraph, der für die Kompilierung von zentraler Bedeutung ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8466040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle einen Workflow-Graphen basierend auf AgentState\n",
    "workflow = StateGraph(AgentState)\n",
    "\n",
    "workflow.add_node(\"Researcher\", research_node)  # Knoten für den Researcher-Agent\n",
    "workflow.add_node(\"summary_agent\", chart_node)  # Knoten für den Zusammenfassungs-Agent\n",
    "workflow.add_node(\"call_tool\", tool_node)  # Knoten für Werkzeugaufrufe\n",
    "\n",
    "# Füge bedingte Übergänge (Conditional Edges) zwischen den Knoten hinzu\n",
    "workflow.add_conditional_edges(\n",
    "    \"Researcher\",  # Ausgangspunkt: Researcher-Agent\n",
    "    router,  # Router-Funktion zur Bestimmung des nächsten Schrittes\n",
    "    {\n",
    "        \"continue\": \"summary_agent\",  # Fortsetzen: Gehe zum Zusammenfassungs-Agent\n",
    "        \"call_tool\": \"call_tool\",  # Werkzeug aufrufen: Gehe zum Tool-Knoten\n",
    "        \"__end__\": END,  # Beenden: Workflow beenden\n",
    "    },\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"summary_agent\",  # Ausgangspunkt: Zusammenfassungs-Agent\n",
    "    router,  # Router-Funktion zur Bestimmung des nächsten Schrittes\n",
    "    {\n",
    "        \"continue\": \"Researcher\",  # Fortsetzen: Gehe zurück zum Researcher-Agent\n",
    "        \"call_tool\": \"call_tool\",  # Werkzeug aufrufen: Gehe zum Tool-Knoten\n",
    "        \"__end__\": END,  # Beenden: Workflow beenden\n",
    "    },\n",
    ")\n",
    "\n",
    "# Bedingte Übergänge für den Werkzeug-Knoten\n",
    "workflow.add_conditional_edges(\n",
    "    \"call_tool\",\n",
    "    # Jeder Agent aktualisiert das Feld 'sender',\n",
    "    # der Werkzeug-Knoten tut dies jedoch nicht. Dadurch\n",
    "    # wird die Nachricht zum ursprünglichen Agenten zurückgeleitet,\n",
    "    # der das Werkzeug aufgerufen hat.\n",
    "    lambda x: x[\"sender\"],  # Basierend auf dem Feld 'sender' wird der nächste Schritt bestimmt\n",
    "    {\n",
    "        \"Researcher\": \"Researcher\",  # Zurück zum Researcher-Agent\n",
    "        \"summary_agent\": \"summary_agent\",  # Zurück zum Zusammenfassungs-Agent\n",
    "    },\n",
    ")\n",
    "\n",
    "# Füge eine Kante hinzu, um den Workflow vom Startpunkt aus zu beginnen\n",
    "workflow.add_edge(START, \"Researcher\")  # Der Workflow startet beim Researcher-Agent\n",
    "\n",
    "# Kompiliere den Workflow-Graphen\n",
    "graph = workflow.compile()  # Bereite den Graphen zur Ausführung vor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c052a770",
   "metadata": {},
   "source": [
    "Der Graph lässt sich auch visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5fef1b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'\\n- `graph.get_graph()`: Ruft den internen Graphen ab, der im Workflow definiert wurde.\\n- `draw_mermaid_png()`: Wandelt den Graphen in eine PNG-Darstellung um, basierend auf Mermaid.js, einem Tool zur Diagrammerstellung.\\n- `display()`: Zeigt die PNG-Datei im Jupyter-Notebook an.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image, display  # Importiert Funktionen zur Anzeige von Bildern in Jupyter-Notebooks\n",
    "\n",
    "# Zeige den Workflow-Graphen als Bild an\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png()  # Erstellt eine PNG-Darstellung des Graphen mit Mermaid.js\n",
    "    )\n",
    ")\n",
    "\"\"\"\n",
    "- `graph.get_graph()`: Ruft den internen Graphen ab, der im Workflow definiert wurde.\n",
    "- `draw_mermaid_png()`: Wandelt den Graphen in eine PNG-Darstellung um, basierend auf Mermaid.js, einem Tool zur Diagrammerstellung.\n",
    "- `display()`: Zeigt die PNG-Datei im Jupyter-Notebook an.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf8b1c",
   "metadata": {},
   "source": [
    "Aufruf des Agenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1725de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_9112\\759352124.py:26: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Researcher': {'messages': [AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_69x7', 'function': {'arguments': '{\"query\":\"latest research on Generative AI Security\"}', 'name': 'search'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 1387, 'total_tokens': 1480, 'completion_time': 0.146440139, 'prompt_time': 0.085695709, 'queue_time': 0.0013969980000000104, 'total_time': 0.232135848}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'tool_calls', 'logprobs': None}, name='Researcher', id='run-be2ee5ee-3276-43e5-bdfa-1b0e896005bc-0', tool_calls=[{'name': 'search', 'args': {'query': 'latest research on Generative AI Security'}, 'id': 'call_69x7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1387, 'output_tokens': 93, 'total_tokens': 1480})], 'sender': 'Researcher'}}\n",
      "----\n",
      "{'call_tool': {'messages': [ToolMessage(content='{\"result\": \"Over the last decade, Artificial Intelligence (AI) has become increasingly popular, especially with the use of chatbots such as ChatGPT, Gemini, and DALL-E. With this rise, large language models (LLMs) and Generative AI (GenAI) have also become more prevalent in everyday use. These advancements strengthen cybersecurity\\'s defensive posture and open up new attack avenues for adversaries as well ... This research paper intends to provide real-life applications of Generative AI (GAI) in the cybersecurity domain. The frequency, sophistication and impact of cyber threats have continued to rise in today\\'s world. This ever-evolving threat landscape poses challenges for organizations and security professionals who continue looking for better solutions to tackle these threats. GAI technology ... This whitepaper highlights the dual importance of securing generative AI (genAI) platforms and leveraging genAI for cybersecurity. As genAI technologies proliferate, their misuse poses significant risks, including data breaches, model tampering, and malicious content generation. Securing these platforms is critical to protect sensitive data, ensure model integrity, and prevent adversarial ... This study conducts a bibliometric analysis on 4,262 works published under the title \\\\\"Generative Artificial Intelligence-GAI\\\\\" between 2020 and 2024, sourced from Web of Science (WoS). The analysis covers keyword clouds, the most frequently used terms in article titles, types of publications, fields with the highest volume of publications. The research on GAI, a rapidly growing subfield of ... According to a recent survey conducted by ISMG, the top concerns for both business executives and security leaders on using generative AI in their organization range, from data security and governance, transparency and accountability to regulatory compliance. 1 In this paper, the first in a series on AI compliance, governance, and safety from the Microsoft Security team, we provide business ...\"}', name='search', tool_call_id='call_69x7')]}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_9112\\759352124.py:26: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Researcher': {'messages': [AIMessage(content='Based on the tool output, the latest research on Generative AI Security highlights the increasing popularity of Generative AI and its applications in cybersecurity. However, the misuse of Generative AI platforms poses significant risks, including data breaches, model tampering, and malicious content generation. Organizations and security professionals must ensure the secure use of these platforms to protect sensitive data, maintain model integrity, and prevent adversarial attacks.\\n\\nA bibliometric analysis of 4,262 works published between 2020 and 2024 reveals that Generative AI is a rapidly growing subfield. Concerns about using Generative AI in organizations range from data security and governance to regulatory compliance.\\n\\nIn summary, while Generative AI offers promising opportunities for cybersecurity, it is crucial to prioritize securing these platforms and address concerns related to data security, governance, and regulatory compliance.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 1891, 'total_tokens': 2083, 'completion_time': 0.304825688, 'prompt_time': 0.134418058, 'queue_time': 0.0015296009999999916, 'total_time': 0.439243746}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, name='Researcher', id='run-7c7a4160-0dd2-4d0d-8144-b8fc526906e4-0', usage_metadata={'input_tokens': 1891, 'output_tokens': 192, 'total_tokens': 2083})], 'sender': 'Researcher'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_9112\\759352124.py:26: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary_agent': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 2080, 'total_tokens': 2081, 'completion_time': 0.0, 'prompt_time': 0.159335897, 'queue_time': -9223372037.014112, 'total_time': 0.159335897}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, name='summary_agent', id='run-71a30664-62e0-4348-af93-f61bae5deada-0', usage_metadata={'input_tokens': 2080, 'output_tokens': 1, 'total_tokens': 2081})], 'sender': 'summary_agent'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_9112\\759352124.py:26: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Researcher': {'messages': [AIMessage(content='I apologize, but I need more context or a specific question to provide a helpful response. I am here to assist you with any questions or tasks you have.\\n\\nIf you\\'d like to know more about Generative AI Security, you can ask a question like, \"Can you tell me more about the security concerns related to Generative AI?\" or \"What are some best practices for securing Generative AI platforms?\"\\n\\nPlease let me know how I can help.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 99, 'prompt_tokens': 2091, 'total_tokens': 2190, 'completion_time': 0.156660772, 'prompt_time': 0.098206228, 'queue_time': 0.0012246069999999887, 'total_time': 0.254867}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, name='Researcher', id='run-980c2d93-f729-4835-b8f7-1baa1b24b6f3-0', usage_metadata={'input_tokens': 2091, 'output_tokens': 99, 'total_tokens': 2190})], 'sender': 'Researcher'}}\n",
      "----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesko.rehberg\\AppData\\Local\\Temp\\ipykernel_9112\\759352124.py:26: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  result = AIMessage(**result.dict(exclude={\"type\", \"name\"}), name=name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary_agent': {'messages': [AIMessage(content='', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 1, 'prompt_tokens': 2196, 'total_tokens': 2197, 'completion_time': 0.0, 'prompt_time': 0.205282182, 'queue_time': -9223372037.060059, 'total_time': 0.205282182}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, name='summary_agent', id='run-757d9fa6-e79b-480a-92ee-6c872037e614-0', usage_metadata={'input_tokens': 2196, 'output_tokens': 1, 'total_tokens': 2197})], 'sender': 'summary_agent'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Starte den Workflow-Graphen und führe die Verarbeitung schrittweise aus\n",
    "events = graph.stream(\n",
    "    {\n",
    "        # Initialer Zustand des Graphen mit einer Nachricht vom Nutzer (HumanMessage)\n",
    "        \"messages\": [\n",
    "            HumanMessage(\n",
    "                content=\"Generate the latest research on Generative AI Security, and summarize a report on the latest research in the field.\"\n",
    "            )\n",
    "        ],\n",
    "    },\n",
    "    # Maximale Anzahl von Schritten, die im Graphen durchgeführt werden dürfen\n",
    "    {\"recursion_limit\": 15},  # Legt ein Limit für die Rekursionstiefe fest, um Endlosschleifen zu vermeiden\n",
    ")\n",
    "\n",
    "# Iteriere über die Ereignisse (Events), die während der Graphenverarbeitung generiert werden\n",
    "for s in events:\n",
    "    print(s)\n",
    "    print(\"----\")  # Trennt die Ereignisse zur besseren Lesbarkeit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ad7d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
